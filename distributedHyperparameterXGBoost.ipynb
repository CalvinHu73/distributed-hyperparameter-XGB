{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "distributedHyperparameterXGBoost.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1NzeQcAYiNB"
      },
      "outputs": [],
      "source": [
        "!conda install mysql -y\n",
        "!python -m pip install mysql\n",
        "!mkdir /opt/conda/data\n",
        "!mysqld --initialize --datadir /opt/conda/data\n",
        "!mysql.server start"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# paste in the generated password\n",
        "!mysqladmin --user=root --password=\"generated password here\" password \"\"\n",
        "!mysql -u root -e \"CREATE DATABASE IF NOT EXISTS example\""
      ],
      "metadata": {
        "id": "BE_ICjJKeDLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna pyarrow==5.0.0\n",
        "# assume role to access S3 bucket\n",
        "!rolypoly assume [put role here]"
      ],
      "metadata": {
        "id": "Hq8Cc_x_eMhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dask\n",
        "import xgboost\n",
        "import optuna\n",
        "from dask import delayed\n",
        "import dask_gateway\n",
        "from dask_gateway import Gateway\n",
        "from dask_ml.model_selection import KFold\n",
        "from xgboost.dask import DaskXGBClassifier as XGBClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import distributed\n",
        "\n",
        "print(dask.__version__, distributed.__version__, xgboost.__version__, dask_gateway.__version__)\n",
        "# 2021.07.0 2021.07.0 1.4.0 0.9.0\n",
        "\n",
        "def initiateCluster():\n",
        "    # adjust cluster configurations as needed\n",
        "    gateway = Gateway()\n",
        "    options = gateway.cluster_options()\n",
        "    options.worker_cores = 4\n",
        "    options.worker_memory = 16\n",
        "    options.scheduler_cores = 4\n",
        "    options.scheduler_memory = 8\n",
        "    options.role = \"put s3 bucket role here so cluster can read from s3\"\n",
        "\n",
        "    cluster = gateway.new_cluster(options)\n",
        "    client = cluster.get_client()\n",
        "    \n",
        "    cluster.scale(16)\n",
        "    return cluster, client\n",
        "\n",
        "def createStudy():\n",
        "    return optuna.create_study(study_name='test_study', direction='maximize', storage=\"mysql://root@localhost/example\", load_if_exists=True)\n",
        "\n",
        "def calcRocAuc(cv_clf, X_val,y_val):\n",
        "        predictions = cv_clf.predict(X_val)\n",
        "        score = roc_auc_score(y_val,predictions)\n",
        "        return score\n",
        "\n",
        "def cv_estimate(X, y, params, n_splits=5):\n",
        "        cv = KFold(n_splits=n_splits)\n",
        "        cv_clf = XGBClassifier(**params)\n",
        "        val_scores = 0\n",
        "        i = 0\n",
        "        for train, val in cv.split(X, y):\n",
        "            print(\"KFold \" + str(i))\n",
        "            start = datetime.datetime.now()\n",
        "            cv_clf.fit(X[train], y[train])\n",
        "            val_scores += calcRocAuc(cv_clf, X[val], y[val])\n",
        "            end = datetime.datetime.now()\n",
        "            print(\"KFold \" + str(i) + \", \" + str(val_scores) + \", \" + str((end - start).total_seconds()))\n",
        "            i += 1\n",
        "        val_scores /= n_splits\n",
        "        return val_scores\n",
        "\n",
        "def objective(trial, X, y):\n",
        "        # change parameter ranges as needed\n",
        "        params = {'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
        "                          'learning_rate': trial.suggest_uniform('learning_rate', 0.01, 0.99),\n",
        "                          'subsample': trial.suggest_uniform('subsample', 0.1, 0.9),\n",
        "                          'max_depth': trial.suggest_int('max_depth', 1, 10),\n",
        "                          'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.1, 0.9),\n",
        "                          'min_child_weight': trial.suggest_int('min_child_weight', 1, 9)\n",
        "        }\n",
        "        accuracy = cv_estimate(X, y, params, 5)\n",
        "        return accuracy"
      ],
      "metadata": {
        "id": "T3uOVoBzePyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mysqlclient\n",
        "!pip install pymysql\n",
        "import pymysql  \n",
        "pymysql.install_as_MySQLdb()\n",
        "import _mysql"
      ],
      "metadata": {
        "id": "i2YVLPEjfIv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import datetime\n",
        "\n",
        "def train_task(identifier):\n",
        "    p_string = \"Process \" + str(identifier)\n",
        "    print(\"Starting \" + p_string)\n",
        "    g = Gateway()\n",
        "    print(p_string + \": Creating cluster...\")\n",
        "    cluster, client = initiateCluster()\n",
        "    print(p_string + \": \" + client.dashboard_link)\n",
        "    \n",
        "    print(p_string + \": Starting XGB fit\")\n",
        "    start = datetime.datetime.now()\n",
        "    \n",
        "    # update chunksize and .rechunk() as needed\n",
        "    X_p = dask.dataframe.read_parquet(\"s3 path to file\", engine=\"pyarrow\", chunksize=30000)\n",
        "    y_p = dask.dataframe.read_parquet(\"s3 path to file\", engine=\"pyarrow\", chunksize=30000) \n",
        "    X = X_p.to_dask_array(lengths=True)\n",
        "    y = y_p.to_dask_array(lengths=True)\n",
        "\n",
        "    X = X.rechunk(30000, 300)\n",
        "    \n",
        "    print(X.compute_chunk_sizes())\n",
        "    print(len(X))\n",
        "    print(len(y))\n",
        "    \n",
        "    study = createStudy()\n",
        "    # change n_trials as needed\n",
        "    study.optimize(lambda trial: objective(trial, X, y), n_trials=2)\n",
        "    \n",
        "    end = datetime.datetime.now()\n",
        "    print(p_string + \" finished train an xgboost classifier\")\n",
        "    print(p_string + \" classifier train time: \" + str((end - start).total_seconds()))\n",
        "\n",
        "from multiprocessing import Process\n",
        "\n",
        "# 2021.07.0 2021.07.0 1.4.0 0.9.0\n",
        "print(dask.__version__, distributed.__version__, xgboost.__version__, dask_gateway.__version__)\n",
        "\n",
        "# change number of processes as needed\n",
        "n = 4\n",
        "\n",
        "# create a number of processes with different clusters\n",
        "processes = [Process(target=train_task, args=(i, )) for i in range(n)]\n",
        "\n",
        "study = createStudy()\n",
        "print('Starting Proccesses')\n",
        "for process in processes:\n",
        "    process.start()\n",
        "    \n",
        "# wait for all processes to finish\n",
        "for process in processes:\n",
        "    process.join()"
      ],
      "metadata": {
        "id": "cmunmxosfI5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"best params\")\n",
        "print(study.best_params)"
      ],
      "metadata": {
        "id": "p1vWg4FBfI8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from time import sleep\n",
        "from dask_gateway import Gateway\n",
        "\n",
        "gateway = Gateway()\n",
        "print(gateway.list_clusters())\n",
        "\n",
        "# close clusters\n",
        "print(\"Closing clusters...\")\n",
        "clusters = gateway.list_clusters()\n",
        "for cluster in clusters:\n",
        "    gateway.connect(cluster.name).shutdown()\n",
        "    \n",
        "while len(gateway.list_clusters()) > 0:\n",
        "    sleep(0.5)\n",
        "    \n",
        "print(\"Finished closing clusters\")\n",
        "gateway.list_clusters()"
      ],
      "metadata": {
        "id": "xDrTJ_tqfI_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "St7lX_VbfhOv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}